load("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/dat/datjss.csv")
load("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/dat/datjss.csv")
datstu <- read.csv("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/dat/datstu.csv",header = TRUE)
View(datstu)
View(datstu)
datsss <- read.csv("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/dat/datsss.csv",header = TRUE)
datjsu <- read.csv("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/dat/datjss.csv",header = TRUE)
View(datsss)
View(datstu)
View(datjsu)
summary(datstu)
inf
Inf
Inf
norm([Inf,Inf])
norm(c(Inf,Inf)
)
norm(c(Inf,Inf),type = 2)
norm(c(Inf,Inf),type = "2")
norm(c(1,1), type="2")
norm(c(Inf,Inf), type="2")
rm(list=ls())
# Set Working directory
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW")
set.seed(100)
# Exercise 1: Data generation ----
obs <- 10000
X1 <- runif(obs, max = 3, min = 1)
X2 <- rgamma(obs,3,scale = 2)
X3 <- rbinom(obs,1,0.3)
eps <- rnorm(obs, mean = 2, sd = 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + eps
ydum <- ifelse(Y > mean(Y),1,0)
mydata <- cbind(Y,ydum,X1,X2,X3,eps)
# Exercise 2: ----
# Correlation between Y and X1
cor(Y,X1)
# The correlation is limited from -1 to 1, so only thing we can tell is that there is a strong
# positive correlation between these two variables, which match the fact that the coef on X1 is possitive.
# Regression of Y on X where X = (1,X1,X2,X3) (Mannually)
ols <- function(X,y,se=F){
n <- nrow(X)
k <- ncol(X)
ols.coef <- solve(t(X)%*%X)%*%(t(X)%*%y) #coefficient
ols.res <- (y-X%*%ols.coef) # residual
ols.V <- 1/(n-k) * as.numeric(t(ols.res)%*%ols.res)*solve(t(X)%*%X) #covariance matrix
ols.se <- as.matrix(sqrt(diag(ols.V))) #standard error
ifelse(se == T, return(data.frame(b_hat = ols.coef,se = ols.se)),return(data.frame(b_hat = ols.coef))) # output coef only by default
}
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)
ols.result <- ols(X,y,se=T)
ols.result
# Check with lm function
lm.result <- lm(Y~X1+X2+X3, data.frame(mydata))
coef(summary(lm.result))[,c(1,2)] #coefficient & standard error
gradient <- jacobian(AME.p, coef(result.p.glm))
cov_matrixv <- vcov(result.p.glm)
se.p <- sqrt(diag(gradient%*%cov_matrixv%*%t(gradient)))
se.p
# Compute the Standard Error by delta method (Logit)
gradient <- jacobian(AME.l, coef(result.l.glm))
cov_matrixv <- vcov(result.l.glm)
se.l <- sqrt(diag(gradient%*%cov_matrixv%*%t(gradient)))
se.l
# Compute the SE of AME by Bootstrap (Probit and Logit)
bootstrapse2 <- function(n,l){
# input bootstrap times and type of link function
boot.me <- data.frame(matrix(nrow=4, ncol=1))[-1]
for (i in 1:n) {
# sample from existing data to get X.s, y.s
df.s <- mydata[sample(nrow(mydata),size = nrow(mydata),replace = T),]
result <- glm(ydum ~ X1 + X2 + X3, family=binomial(link = l),data.frame(df.s))
# calculate ols coefficient in each sample
me <- mean(dnorm(predict.glm(result, type = "link")))*coef(result)
boot.me <- cbind(boot.me,me)
}
# Report the SE over all the obtained coefficients
return(data.frame(se = apply(boot.me,1,sd)))
}
bootstrapse2(49,"probit")
bootstrapse2(49,"logit")
install.packages("margins")
library("margins")
result.p.glm <- glm(ydum ~ X1 + X2 + X3, family = binomial(link = "probit"),
data = data.frame(mydata))
coef(summary(result.p.glm))
marginal_effects(result.r.glm)
marginal_effects(result.p.glm)
m <- margins(result.p.glm)
summary(m)
AME.p <- function(result) mean(dnorm(X%*%result))*result # result are the parameters
AME.p(coef(result.p.glm))
se.p
gradient <- jacobian(AME.p, coef(result.p.glm))
cov_matrixv <- vcov(result.p.glm)
se.p <- sqrt(diag(gradient%*%cov_matrixv%*%t(gradient)))
library("numDeriv")
gradient <- jacobian(AME.p, coef(result.p.glm))
cov_matrixv <- vcov(result.p.glm)
se.p <- sqrt(diag(gradient%*%cov_matrixv%*%t(gradient)))
se.p

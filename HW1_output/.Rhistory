library(Rmisc)
library(dplyr)
library(nnet)
library(fastDummies)
datapath = "/dat"
modpath  = "/Users/ms486/Dropbox/Teaching/2019/AppliedEconometrics/Econ613/A1"
options(xtable.floating = FALSE)
#========================================================
# load the data
#========================================================
datstu = read.csv(paste0(modpath,datapath,"/","datstu.csv"))
datjss = read.csv(paste0(modpath,datapath,"/","datjss.csv"))
datsss = read.csv(paste0(modpath,datapath,"/","datsss.csv"))
#========================================================
# Exercise 1
#========================================================
str(datstu) # provide a good overview of the data
#====================
#number of students
#====================
nrow(datstu)
#====================
#number of schools
#====================
# here i find all the variables with schoolcode(grep(schoolcode)), vectorize it,
# then find unique set of schools, and then length it
# get rid of the missing observations
length(unique(c(as.matrix(datstu[,grep("schoolcode",names(datstu))])),na.rm=T))
#number of programs
length(unique(c(as.matrix(datstu[,grep("pgm",names(datstu))])),na.rm=T))
#number of choices
datstu$choice1  = paste0(datstu$schoolcode1,datstu$choicepgm1)
datstu$choice2  = paste0(datstu$schoolcode2,datstu$choicepgm2)
datstu$choice3  = paste0(datstu$schoolcode3,datstu$choicepgm3)
datstu$choice4  = paste0(datstu$schoolcode4,datstu$choicepgm4)
datstu$choice5  = paste0(datstu$schoolcode5,datstu$choicepgm5)
datstu$choice6  = paste0(datstu$schoolcode6,datstu$choicepgm6)
nc = ncol(datstu)
length(unique(c(as.matrix(datstu[,(nc-5):nc])),na.rm=T))
#missing test score
table(is.na(datstu$score))
#apply to the same school
schools = apply(as.matrix(datstu[,grep("schoolcode",names(datstu))]),1,function(x)length(unique((x))))
table(schools)
#apply to less than 6 choices
table(is.na(dat08$schoolcode1))
table(is.na(dat08$schoolcode2))
table(is.na(dat08$schoolcode3))
table(is.na(dat08$schoolcode4))
table(is.na(dat08$schoolcode5))
table(is.na(dat08$schoolcode6))
rm(list=ls())
# Set Working directory
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW/HW1_output")
#================================================================
# Solution for assignment 1
#================================================================
rm(list=ls())
library(foreign)
library(weights)
library(ggplot2)
library(gridExtra)
library(reshape)
library(gdata)
library(Rmisc)
library(pROC)
library(grid)
library(readstata13)
library(xtable)
library(Rmisc)
library(dplyr)
library(nnet)
library(fastDummies)
options(xtable.floating = FALSE)
#========================================================
# load the data
#========================================================
# Reading the CSV files
datstu <- read.csv("datstu.csv",header = TRUE)
datsss <- read.csv("datsss.csv",header = TRUE)
datjsu <- read.csv("datjss.csv",header = TRUE)
#========================================================
# Exercise 1
#========================================================
str(datstu) # provide a good overview of the data
#====================
#number of students
#====================
nrow(datstu)
#====================
#number of schools
#====================
# here i find all the variables with schoolcode(grep(schoolcode)), vectorize it,
# then find unique set of schools, and then length it
# get rid of the missing observations
length(unique(c(as.matrix(datstu[,grep("schoolcode",names(datstu))])),na.rm=T))
#number of programs
length(unique(c(as.matrix(datstu[,grep("pgm",names(datstu))])),na.rm=T))
#number of choices
datstu$choice1  = paste0(datstu$schoolcode1,datstu$choicepgm1)
datstu$choice2  = paste0(datstu$schoolcode2,datstu$choicepgm2)
datstu$choice3  = paste0(datstu$schoolcode3,datstu$choicepgm3)
datstu$choice4  = paste0(datstu$schoolcode4,datstu$choicepgm4)
datstu$choice5  = paste0(datstu$schoolcode5,datstu$choicepgm5)
datstu$choice6  = paste0(datstu$schoolcode6,datstu$choicepgm6)
nc = ncol(datstu)
length(unique(c(as.matrix(datstu[,(nc-5):nc])),na.rm=T))
#missing test score
table(is.na(datstu$score))
#apply to the same school
schools = apply(as.matrix(datstu[,grep("schoolcode",names(datstu))]),1,function(x)length(unique((x))))
table(schools)
#apply to less than 6 choices
table(is.na(dat08$schoolcode1))
table(is.na(dat08$schoolcode2))
table(is.na(dat08$schoolcode3))
table(is.na(dat08$schoolcode4))
table(is.na(dat08$schoolcode5))
table(is.na(dat08$schoolcode6))
View(datstu)
table(is.na(datstu$schoolcode1))
table(is.na(datstu$schoolcode2))
table(is.na(datstu$schoolcode3))
table(is.na(datstu$schoolcode4))
table(is.na(datstu$schoolcode5))
table(is.na(datstu$schoolcode6))
17088+17140+406+195+406+195+163+102
missing <- sum(is.na(datstu$score))
missing
View(datstu)
is.na(datstu$score)
-is.na(datstu$score)
is.na(datstu$score)
tail(is.na(datstu$score))
datchoice = datstu[,(nc-5):nc]
placement = NULL
for (iter in 1:nrow(datstu))
{
if (!is.na(datstu$rankplace[iter])&datstu$rankplace[iter]<7)
{
placement[iter] = datchoice[iter,datstu$rankplace[iter]]
}
}
#====================================================================
# Second, i create dataframe indicating for each student
# the test score and the school he got admitted to..
# then I cast that database into a dataframe indicating the cutoff and quality
# then I extract the first 6 characteristics of the choice to get the school code
#====================================================================
temp        = data.frame(placement,datstu$score)
names(temp) = c("school","score")
dat_prog  =  temp %>%
group_by(school) %>%
summarise(cutoff  = min(score,na.rm=T),
quality = mean(score,na.rm=T))
datfinal            = data.frame(dat_prog)
datfinal$schoolcode = substr(datfinal$school,1,6)
datfinal[1:10,]
#====================================================================
# Finally, i match this dataframe to sss by schoolcode
#====================================================================
datf          = merge(datfinal,datsss,by="schoolcode")
dat_aggregate = subset(datf,select=c("school","cutoff","quality","ssslong","ssslat"))
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW/HW2_output")
set.seed(613)
obs <- 10000
X1 <- runif(obs, max = 3, min = 1)
X2 <- rgamma(obs,3,scale = 2)
X3 <- rbinom(obs,1,0.3)
eps <- rnorm(obs, mean = 2, sd = 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + eps
ydum <- ifelse(Y > mean(Y),1,0)
mydata <- data.frame(cbind(Y,ydum,X1,X2,X3,eps))
cor(Y,X1)
ols <- function(df,se=F){
# allow dataframe input
X <- as.matrix(cbind(1,df[,grep("X",colnames(df))]))
y <- df[,grep("Y",colnames(df))]
n <- nrow(X)
k <- ncol(X)
ols.coef <- solve(t(X)%*%X)%*%(t(X)%*%y) #coefficient
ols.res <- (y-X%*%ols.coef) # residual
ols.V <- 1/(n-k) * as.numeric(t(ols.res)%*%ols.res)*solve(t(X)%*%X) #covariance matrix
ols.se <- as.matrix(sqrt(diag(ols.V))) #standard error
ifelse(se == T, return(data.frame(b_hat = ols.coef,se = ols.se)),return(data.frame(b_hat = ols.coef))) # output coef only by default
}
ols.result <- ols(mydata,se=T)
ols.result
lm.result <- lm(Y~X1+X2+X3, mydata)
coef(summary(lm.result))[,c(1,2)] #coefficient & standard error
bootstrapse <- function(n,fun){
# Input times of bootstrap
boot.result <- data.frame(result = NA)[-1] # creating empty data frame
for (i in 1:n) {
# sample from existing data
df.s <- mydata[sample(nrow(mydata),size = nrow(mydata),replace = T),]
# calculate the result based on input function
boot.result <- cbind(boot.result,fun(df.s))
}
# Report the SE over all the obtained result
return(data.frame(se = apply(boot.result,1,sd)))
}
boot.se_49 <- bootstrapse(49,ols)
boot.se_499 <- bootstrapse(499,ols)
boot.se_49
boot.se_499
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)
probit.llike <- function(b., y. = ydum,X. = X){
phi <- pnorm(X.%*%b.)
phi[phi==1] <- 0.9999 # avoid NaN of log function
phi[phi==0] <- 0.0001
f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
f <- -f
return(f)
}
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
return(J)
}
jacobian(probit.llike,c(0.5,0.5,0.5,0.5))
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW/HW2_output")
set.seed(1)
obs <- 10000
X1 <- runif(obs, max = 3, min = 1)
X2 <- rgamma(obs,3,scale = 2)
X3 <- rbinom(obs,1,0.3)
eps <- rnorm(obs, mean = 2, sd = 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + eps
ydum <- ifelse(Y > mean(Y),1,0)
mydata <- data.frame(cbind(Y,ydum,X1,X2,X3,eps))
cor(Y,X1)
ols <- function(df,se=F){
# allow dataframe input
X <- as.matrix(cbind(1,df[,grep("X",colnames(df))]))
y <- df[,grep("Y",colnames(df))]
n <- nrow(X)
k <- ncol(X)
ols.coef <- solve(t(X)%*%X)%*%(t(X)%*%y) #coefficient
ols.res <- (y-X%*%ols.coef) # residual
ols.V <- 1/(n-k) * as.numeric(t(ols.res)%*%ols.res)*solve(t(X)%*%X) #covariance matrix
ols.se <- as.matrix(sqrt(diag(ols.V))) #standard error
ifelse(se == T, return(data.frame(b_hat = ols.coef,se = ols.se)),return(data.frame(b_hat = ols.coef))) # output coef only by default
}
ols.result <- ols(mydata,se=T)
ols.result
lm.result <- lm(Y~X1+X2+X3, mydata)
coef(summary(lm.result))[,c(1,2)] #coefficient & standard error
bootstrapse <- function(n,fun){
# Input times of bootstrap
boot.result <- data.frame(result = NA)[-1] # creating empty data frame
for (i in 1:n) {
# sample from existing data
df.s <- mydata[sample(nrow(mydata),size = nrow(mydata),replace = T),]
# calculate the result based on input function
boot.result <- cbind(boot.result,fun(df.s))
}
# Report the SE over all the obtained result
return(data.frame(se = apply(boot.result,1,sd)))
}
boot.se_49 <- bootstrapse(49,ols)
boot.se_499 <- bootstrapse(499,ols)
boot.se_49
boot.se_499
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)
probit.llike <- function(b., y. = ydum,X. = X){
phi <- pnorm(X.%*%b.)
phi[phi==1] <- 0.9999 # avoid NaN of log function
phi[phi==0] <- 0.0001
f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
f <- -f
return(f)
}
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
return(J)
}
jacobian(probit.llike,c(0.5,0.5,0.5,0.5))
```{r}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW/HW2_output")
set.seed(1)
obs <- 10000
X1 <- runif(obs, max = 3, min = 1)
X2 <- rgamma(obs,3,scale = 2)
X3 <- rbinom(obs,1,0.3)
eps <- rnorm(obs, mean = 2, sd = 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + eps
ydum <- ifelse(Y > mean(Y),1,0)
mydata <- data.frame(cbind(Y,ydum,X1,X2,X3,eps))
cor(Y,X1)
ols <- function(df,se=F){
# allow dataframe input
X <- as.matrix(cbind(1,df[,grep("X",colnames(df))]))
y <- df[,grep("Y",colnames(df))]
n <- nrow(X)
k <- ncol(X)
ols.coef <- solve(t(X)%*%X)%*%(t(X)%*%y) #coefficient
ols.res <- (y-X%*%ols.coef) # residual
ols.V <- 1/(n-k) * as.numeric(t(ols.res)%*%ols.res)*solve(t(X)%*%X) #covariance matrix
ols.se <- as.matrix(sqrt(diag(ols.V))) #standard error
ifelse(se == T, return(data.frame(b_hat = ols.coef,se = ols.se)),return(data.frame(b_hat = ols.coef))) # output coef only by default
}
ols.result <- ols(mydata,se=T)
ols.result
lm.result <- lm(Y~X1+X2+X3, mydata)
coef(summary(lm.result))[,c(1,2)] #coefficient & standard error
bootstrapse <- function(n,fun){
# Input times of bootstrap
boot.result <- data.frame(result = NA)[-1] # creating empty data frame
for (i in 1:n) {
# sample from existing data
df.s <- mydata[sample(nrow(mydata),size = nrow(mydata),replace = T),]
# calculate the result based on input function
boot.result <- cbind(boot.result,fun(df.s))
}
# Report the SE over all the obtained result
return(data.frame(se = apply(boot.result,1,sd)))
}
boot.se_49 <- bootstrapse(49,ols)
boot.se_499 <- bootstrapse(499,ols)
boot.se_49
boot.se_499
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)
probit.llike <- function(b., y. = ydum,X. = X){
phi <- pnorm(X.%*%b.)
phi[phi==1] <- 0.9999 # avoid NaN of log function
phi[phi==0] <- 0.0001
f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
f <- -f
return(f)
}
result.p <- optim(par = c(0,0,0,0), probit.llike)
result.p$par
graddes <- function(b,stop,fun){
alpha <- 0.01 # any initial alpha
delta <- Inf
while (delta > stop){
# Calculate gradient for each regressor by Finite Difference Method
g <- jacobian(probit.llike,b)
# Backtracking (Armijo–Goldstein condition tests to make sure the size of alpha)
if(fun(b - alpha*g)>fun(b)-0.1*alpha*t(g)%*%g){
alpha <- 0.5*alpha
next
}
# Make step forward (t+1) and saved in bn
bn <- b - alpha*g
# Stopping Criteria
delta <- (abs(fun(bn)-fun(b))/abs(fun(b)))
b <- bn
}
return(b)
}
result.gd <- graddes(c(0,0,0,0),1e-5,probit.llike)
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
return(J)
}
graddes <- function(b,stop,fun){
alpha <- 0.01 # any initial alpha
delta <- Inf
while (delta > stop){
# Calculate gradient for each regressor by Finite Difference Method
g <- jacobian(probit.llike,b)
# Backtracking (Armijo–Goldstein condition tests to make sure the size of alpha)
if(fun(b - alpha*g)>fun(b)-0.1*alpha*t(g)%*%g){
alpha <- 0.5*alpha
next
}
# Make step forward (t+1) and saved in bn
bn <- b - alpha*g
# Stopping Criteria
delta <- (abs(fun(bn)-fun(b))/abs(fun(b)))
b <- bn
}
return(b)
}
result.gd <- graddes(c(0,0,0,0),1e-5,probit.llike)
result.gd
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
print(apply(par. + diag(d,length(par)),2,fun))
return(J)
}
jacobian(probit.llike,c(0,0,0,0))
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
print(apply(par. + diag(d,length(par)),2,fun))
print(apply(par.,2,fun))
return(J)
}
jacobian(probit.llike,c(0,0,0,0))
fun = probit.llike()
fun = probit.llike
par = c(0,0,0,0)
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
d <- 1e-8
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
jacobian(probit.llike,c(1,1,1,1))
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
return(J)
}
jacobian(probit.llike,c(1,1,1,1))
probit.llike(c(1,1,1,1))
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW/HW2_output")
set.seed(1)
obs <- 10000
X1 <- runif(obs, max = 3, min = 1)
X2 <- rgamma(obs,3,scale = 2)
X3 <- rbinom(obs,1,0.3)
eps <- rnorm(obs, mean = 2, sd = 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + eps
ydum <- ifelse(Y > mean(Y),1,0)
mydata <- data.frame(cbind(Y,ydum,X1,X2,X3,eps))
cor(Y,X1)
ols <- function(df,se=F){
# allow dataframe input
X <- as.matrix(cbind(1,df[,grep("X",colnames(df))]))
y <- df[,grep("Y",colnames(df))]
n <- nrow(X)
k <- ncol(X)
ols.coef <- solve(t(X)%*%X)%*%(t(X)%*%y) #coefficient
ols.res <- (y-X%*%ols.coef) # residual
ols.V <- 1/(n-k) * as.numeric(t(ols.res)%*%ols.res)*solve(t(X)%*%X) #covariance matrix
ols.se <- as.matrix(sqrt(diag(ols.V))) #standard error
ifelse(se == T, return(data.frame(b_hat = ols.coef,se = ols.se)),return(data.frame(b_hat = ols.coef))) # output coef only by default
}
ols.result <- ols(mydata,se=T)
ols.result
lm.result <- lm(Y~X1+X2+X3, mydata)
coef(summary(lm.result))[,c(1,2)] #coefficient & standard error
bootstrapse <- function(n,fun){
# Input times of bootstrap
boot.result <- data.frame(result = NA)[-1] # creating empty data frame
for (i in 1:n) {
# sample from existing data
df.s <- mydata[sample(nrow(mydata),size = nrow(mydata),replace = T),]
# calculate the result based on input function
boot.result <- cbind(boot.result,fun(df.s))
}
# Report the SE over all the obtained result
return(data.frame(se = apply(boot.result,1,sd)))
}
boot.se_49 <- bootstrapse(49,ols)
boot.se_499 <- bootstrapse(499,ols)
boot.se_49
boot.se_499
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)
probit.llike <- function(b., y. = ydum,X. = X){
phi <- pnorm(X.%*%b.)
phi[phi==1] <- 0.9999 # avoid NaN of log function
phi[phi==0] <- 0.0001
f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
f <- -f
return(f)
}
jacobian <- function(fun,par){
d <- 1e-8
par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
return(J)
}
probit.llike(c(1,1,1,1))
View(X)
tail(pnorm(X%*%c(1,1,1,1)))
phi <- pnorm(X%*%c(1,1,1,1)))
phi <- pnorm(X%*%c(1,1,1,1))
phi[phi==1] <- 0.9999 # avoid NaN of log function
phi[phi==0] <- 0.0001
f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
f <- sum(y*log(phi))+sum((1-y)*log(1-phi))
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)
probit.llike <- function(b., y. = ydum,X. = X){
phi <- pnorm(X.%*%b.)
phi[phi==1] <- 0.9999 # avoid NaN of log function
phi[phi==0] <- 0.0001
f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
f <- -f
return(f)
}
probit.llike(c(1,1,1,1))
result.p.glm <- glm(ydum ~ X1 + X2 + X3, family = binomial(link = "probit"),
data = mydata)
coef(summary(result.p.glm))
result.l.glm <- glm(ydum ~ X1 + X2 + X3, family = binomial(link = "logit"),
data = mydata)
coef(summary(result.l.glm))

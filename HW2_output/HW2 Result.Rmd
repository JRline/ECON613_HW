---
title: "HW2 Result"
author: "Jie Ren"
date: "February 8, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r initialize}
rm(list=ls())
setwd("C:/Users/jiere/Dropbox/Spring 2019/ECON 613/ECON613_HW/HW2_output")
set.seed(613)
```

### Exercise 1: Data generation ---
```{r}
obs <- 10000
X1 <- runif(obs, max = 3, min = 1)
X2 <- rgamma(obs,3,scale = 2)
X3 <- rbinom(obs,1,0.3)
eps <- rnorm(obs, mean = 2, sd = 1)
Y <- 0.5 + 1.2*X1 - 0.9*X2 + 0.1*X3 + eps
ydum <- ifelse(Y > mean(Y),1,0)
mydata <- data.frame(cbind(Y,ydum,X1,X2,X3,eps))
```
### Exercise 2: ---
#### Correlation between Y and X1
```{r}
cor(Y,X1)
```
The correlation is limited from -1 to 1, so only thing we can tell is that there is a strong positive correlation between these two variables, which match the fact that the coef on X1 is positive.

#### Regression of Y on X where X = (1,X1,X2,X3) (Mannually)
```{r paged.print=FALSE}
ols <- function(df,se=F){
  # allow dataframe input
  X <- as.matrix(cbind(1,df[,grep("X",colnames(df))]))
  y <- df[,grep("Y",colnames(df))]
  n <- nrow(X)
  k <- ncol(X)
  ols.coef <- solve(t(X)%*%X)%*%(t(X)%*%y) #coefficient
  ols.res <- (y-X%*%ols.coef) # residual
  ols.V <- 1/(n-k) * as.numeric(t(ols.res)%*%ols.res)*solve(t(X)%*%X) #covariance matrix
  ols.se <- as.matrix(sqrt(diag(ols.V))) #standard error
  ifelse(se == T, return(data.frame(b_hat = ols.coef,se = ols.se)),return(data.frame(b_hat = ols.coef))) # output coef only by default
}

ols.result <- ols(mydata,se=T)
ols.result
```
#### Check with lm function
```{r}
lm.result <- lm(Y~X1+X2+X3, mydata)
coef(summary(lm.result))[,c(1,2)] #coefficient & standard error
```
#### Boodstrap mannually
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
bootstrapse <- function(n,fun){
  # Input times of bootstrap
  boot.result <- data.frame(result = NA)[-1] # creating empty data frame
  for (i in 1:n) {
    # sample from existing data
    df.s <- mydata[sample(nrow(mydata),size = nrow(mydata),replace = T),]
    # calculate the result based on input function
    boot.result <- cbind(boot.result,fun(df.s))
  }
  # Report the SE over all the obtained result
  return(data.frame(se = apply(boot.result,1,sd)))
}

boot.se_49 <- bootstrapse(49,ols)
boot.se_499 <- bootstrapse(499,ols)
boot.se_49
boot.se_499
```
### Exercise 3: ---
#### likelihood function
```{r}
X <- cbind(1,X1,X2,X3)
y <- as.matrix(Y)

probit.llike <- function(b. = b, y. = ydum,X. = X){
  phi <- pnorm(X.%*%b.)
  phi[phi==1] <- 0.9999 # avoid NaN of log function
  phi[phi==0] <- 0.0001
  f <- sum(y.*log(phi))+sum((1-y.)*log(1-phi))
  f <- -f
  return(f)
}
```
#### Optimizer using steepest descent
First define a function to calculate gradient/jacobian by finite difference method
```{r}
jacobian <- function(fun,par){
  d <- 1e-8
  par. <- matrix(par,length(par),length(par)) # generate a matrix that repeating par vector
  J <- (apply(par. + diag(d,length(par)),2,fun)-apply(par.,2,fun))/d
  return(J)
}
```
By input a initial guess of parameter, relative stopping criteria (percentage change in function), and function, using gradient decent method featuring backtracking line search, you can get the parameter that minimize the function. Note: First argument of fun. must be the parameter you want to get, X and y must be set into the default value.
```{r}
graddes <- function(b,stop,fun){
  alpha <- 0.01 # any initial alpha
  delta <- Inf
  while (delta > stop){
    # Calculate gradient for each regressor by Finite Difference Method
    g <- jacobian(probit.llike,b)
    # Backtracking (Armijoâ€“Goldstein condition tests to make sure the size of alpha)
    if(fun(b - alpha*g)>fun(b)-0.1*alpha*t(g)%*%g){
      alpha <- 0.5*alpha
      next
    }
    # Make step forward (t+1) and saved in bn
    bn <- b - alpha*g 
    # Stopping Criteria 
    delta <- (abs(fun(bn)-fun(b))/abs(fun(b)))
    b <- bn
  }
  return(b)
}
result.gd <- graddes(c(0,0,0,0),1e-5,probit.llike)
result.gd
```
Except the coefficient on constant, others are close, but still have a big difference.

### Exercise 4 ---
#### Optimize Probit
```{r}
b.p <- c(0,0,0,0)
result.p <- optim(par = b.p, probit.llike)
result.p$par
```

#### Optimizing Logit
```{r}
logit.llike <- function(b., y. = ydum,X. = X){
  gamma <- plogis(X%*%b.)
  f <- sum(y.*log(gamma))+sum((1-y.)*log(1-gamma))
  f <- -f
  return(f)
}
b.l <- c(0,0,0,0)
result.l <- optim(par = b.l, logit.llike)
result.l$par
```
#### Optimizing Linear Probability
```{r}
result.lp <- lm(ydum~X1+X2+X3)
summary(result.lp)
```
#### Check significance with glm function
```{r message=FALSE, warning=FALSE}
result.p.glm <- glm(ydum ~ X1 + X2 + X3, family = binomial(link = "probit"), 
                data = mydata)
coef(summary(result.p.glm))

result.l.glm <- glm(ydum ~ X1 + X2 + X3, family = binomial(link = "logit"), 
                data = mydata)
coef(summary(result.l.glm))
```
The coefficient varies largely across these three methods, but the sign on the coefficients are the same, and all X3 coefs are not significant.For Probit and logit, without calculating the marginal effect, we can only interpret the sign. Higher X2 can decrease the probability of ydum = 1, which means higher X2 is likely to generate Y below mean. Higher X1 and X3 can increase the probability of ydum = 1, which means that higher X1 and X3 is likely to generate Y above mean. This match the sign in the data generating process. For linear probability model, X1: one unit increase in X1 likely to increase the likelihood of ydum = 1 by 14%; X2: one unit increase in X2 likely to decrease the likelihood of ydum = 1 by 10%; X3: one unit increase in X3 likely to increase the likelihood of ydum = 1 by 1%.

### Exercise 5 ---
#### Compute Average Marginal Effect (AME) of X of probit
```{r}
AME.p <- function(result) mean(dnorm(X%*%result))*result # result are the parameters
AME.p(coef(result.p.glm))
```
#### Compute Average Margeinal Effect of X of Logit
```{r}
AME.l <- function(result) mean(dlogis(X%*%result))*result
AME.l(coef(result.l.glm))
```
Comment: Except the constant term, both models' average marginal effects are close to linear probability

#### Compute the Standard Error of AME by delta method (Probit)
```{r warning=FALSE}
J <- jacobian(AME.p, coef(result.p.glm)) # "jacobian" defined in EX3
cov_matrixv <- vcov(result.p.glm)
se.p <- sqrt(diag(J%*%cov_matrixv%*%t(J)))
se.p
```
#### Compute the Standard Error of AME by delta method (Logit)
```{r}
J <- jacobian(AME.l, coef(result.l.glm))
cov_matrixv <- vcov(result.l.glm)
se.l <- sqrt(diag(J%*%cov_matrixv%*%t(J)))
se.l
```
#### Compute the SE of AME by Bootstrap (Probit and Logit)
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Write a function that can directly derive AME from dataframe
probit.AME <- function(df){
  result <- glm(ydum ~ X1 + X2 + X3, family=binomial(link = "probit"),df)
  return(AME.p(coef(result)))
}
logit.AME <- function(df){
  result <- glm(ydum ~ X1 + X2 + X3, family=binomial(link = "logit"),df)
  return(AME.l(coef(result)))
}
# Plug in the prewrited "bootstrapese"
bootstrapse(49,probit.AME)
bootstrapse(49,logit.AME)
```
